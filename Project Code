Created on Sun Nov 20 23:46:08 2022

@author: Odinakachukwu
"""
#Converted the PDF document to Text File

import PyPDF2
import pdfplumber
file = "BuildingDoc.pdf"
pdfobject= open(file,"rb")
pdfreader = PyPDF2.PdfFileReader(pdfobject)
pages=pdfreader.numPages
print(pages)

with pdfplumber.open(file) as pdf:
    for i in range(0,pages):
        page = pdf.pages[i]
        print(f"extract page number:{page}")
        text = f"{page.extract_text()}\n"
        book = open("HMBulding.text", "a",encoding='utf-8')
        book.writelines(text)
        print("saved to text")
book.close()


import nltk
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd 
import re
from collections import Counter


#read the converted text into python
with open(r'C:\Users\Lenovo 2022\HMBulding.text', "r", encoding='utf-8', errors='ignore') as f:
    contents = f.read()
#The corpus was broken down into sentences
corpus = nltk.sent_tokenize(contents)
#print(corpus)

#Preprocessing Commenced using regular expression
for i in range(len(corpus)):
    corpus [i] = corpus [i].lower()    #converts all Uppercase letters to small letters
    corpus [i] = re.sub(r'\W',' ',corpus [i])  #removed empty spaces
    corpus [i] = re.sub(r'\s+',' ',corpus [i]) #removed punctuation marks
    corpus [i] = re.sub(r'[^A-Za-z]',' ',corpus [i]) #removed special characters like @
    shortword = re.compile(r'\W*\b\w{1}\b') # remove short words
    corpus [i] = re.sub(shortword,' ',corpus [i])
#print(corpus)


#removing stop words from corpus and saving output text to file
from nltk.corpus import stopwords
stopwords.words('english')
stop_words = set(stopwords.words('english'))
file1=open("HMbuildingclean.text","a",encoding='utf-8')
for sentence in corpus:
    tokens = nltk.word_tokenize(sentence)  #Tokenization
    for token in tokens:
        token=''.join([i for i in token if not i.isdigit()])
        if not token in stop_words:
            file1.write(" "+token)
file1.close()

#Applying our word2vec model
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
from gensim import models, matutils
from gensim.models import Word2Vec


input_filename= r'C:\Users\Lenovo 2022\HMbuildingclean.text' #text file to train on
model_filename= r'C:\Users\Lenovo 2022\HMbuildingclean.model'# name for saving trained model

#train using skip gram
skip_gram=True

#create vocabulary
print('building vocabulary...')
model = models.Word2Vec()
sentences = models.word2vec.LineSentence(input_filename)
model.build_vocab(sentences)
bigram_transformer = models.Phrases(sentences)
model = models.Word2Vec(bigram_transformer[sentences], vector_size=200, min_count=5, window=6)



#train word2vec model
print('training model....')
if skip_gram:
    model.train(bigram_transformer[sentences], total_examples = model.corpus_count, epochs = 5)
else:
    model.train(bigram_transformer[sentences], total_examples =model.corpus_count, epochs =model.iter,vector_size=200, min_count=5, window=6)

#and save the trained model
print('_saving model...')
model.save(model_filename)

#Exiting
print('Done!')

#testing word2vec word relationships
print(model.wv.most_similar(positive=['foundation','excavated'],negative=['ceiling']))

#Printing the total words in the model
print(model.corpus_total_words)

#visualizing the vector representation around the word 'heating'
print(model.wv.similar_by_vector('heating'))

#visualizing the words around heating
print(model.wv.similar_by_word('heating'))

#end for word2vec

#Interactive tools for Matplotlib to visualize word vectors in a plot
import numpy as np
import matplotlib.pyplot as plt

from sklearn.manifold import TSNE
from gensim.models import Word2Vec

print('loading the model.....')

model_filename= r'C:\Users\Lenovo 2022\HMbuildingclean.model'

model= Word2Vec.load(model_filename)

def display_closestwords_tsnescatterplot(model, word):
    
    arr = np.empty((0,200), dtype='f')
    word_labels = [word]
    
    #get close words
    close_words = model.wv.most_similar(word)
    
    #add the vector for each of the closest words in the array
    arr = np.append(arr, np.array(model.wv.__getitem__([word])), axis=0)
    for wrd_score in close_words:
        wrd_vector = model.wv.__getitem__([wrd_score[0]])
        word_labels.append(wrd_score[0])
        arr=np.append(arr, np.array(model.wv.__getitem__([word])), axis=0)
        
        # find tsne coords for 2 dimensions
    
    tsne = TSNE(n_components=2, random_state=0)
    np.set_printoptions(suppress=True)
    Y = tsne.fit_transform(arr)
    
    x_coords = Y[:, 0]
    y_coords = Y[:, 1]
    
    # Display scatter plot
    plt.scatter(x_coords, y_coords)
    
    for label, x, y, in zip(word_labels, x_coords, y_coords):
        plt.annotate(label, xy=(x,y), xytext=(0, 0), textcoords='offset points')
    plt.xlim(x_coords.min()+0.00009, x_coords.max()+0.00009)
    plt.ylim(y_coords.min()+0.00009, y_coords.max()+0.00009)
    plt.show()

display_closestwords_tsnescatterplot(model, 'roof')  #plots the words around roof

#BERT

from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')
unmasker("code of practice workmanship on [MASK] sites")


#Fine Tuned BERT

import tensorflow as tf
import tensorflow_text as tf_text
import tensorflow_text as text
import functools
import os

import numpy as np
import matplotlib.pyplot as plt

import nltk
import tensorflow_models as tfm
import tensorflow_hub as hub
import tensorflow_datasets as tfds
tfds.disable_progress_bar()




import requests
from transformers import BertTokenizer, BertForPreTraining
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForPreTraining.from_pretrained('bert-base-uncased')



with open(r'C:\Users\Lenovo 2022\HMBulding.text', "r", encoding='utf-8', errors='ignore') as f:
    contents = f.read()
corpus = nltk.sent_tokenize(contents)

print(corpus[:5])  

text=corpus

bag = [item for sentence in text for item in sentence.split('.') if item != '']
bag_size = len(bag)
print(bag_size)
text[35]    
bag[29:36]

import random

sentence_a = []
sentence_b = []
label = []

for paragraph in text:
    sentences = [
        sentence for sentence in paragraph.split('.') if sentence != ''
    ]
    num_sentences = len(sentences)
    if num_sentences > 1:
        start = random.randint(0, num_sentences-2)
        # 50/50 whether is IsNextSentence or NotNextSentence
        if random.random() >= 0.5:
            # this is IsNextSentence
            sentence_a.append(sentences[start])
            sentence_b.append(sentences[start+1])
            label.append(0)
        else:
            index = random.randint(0, bag_size-1)
            # this is NotNextSentence
            sentence_a.append(sentences[start])
            sentence_b.append(bag[index])
            label.append(1)

for i in range(3):
    print(label[i])
    print(sentence_a[i] + '\n---')
    print(sentence_b[i] + '\n')

inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt',
                   max_length=512, truncation=True, padding='max_length')

inputs.keys()
inputs

inputs['next_sentence_label'] = torch.LongTensor([label]).T


inputs.next_sentence_label[:10]

inputs['labels'] = inputs.input_ids.detach().clone()
inputs.keys()


# create random array of floats with equal dimensions to input_ids tensor
rand = torch.rand(inputs.input_ids.shape)
# create mask array
mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \
           (inputs.input_ids != 102) * (inputs.input_ids != 0)

selection = []

for i in range(inputs.input_ids.shape[0]):
    selection.append(
        torch.flatten(mask_arr[i].nonzero()).tolist()
    )
selection[:2]

for i in range(inputs.input_ids.shape[0]):
    inputs.input_ids[i, selection[i]] = 103
    
inputs.keys()

inputs.input_ids


class OurDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def __len__(self):
        return len(self.encodings.input_ids)
    
dataset = OurDataset(inputs)


loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

model.to(device)

model.train()

from transformers import AdamW

optim = AdamW(model.parameters(), lr=1e-5)

from tqdm import tqdm  # for our progress bar




epochs = 2

for epoch in range(epochs):
    # setup loop with TQDM and dataloader
    loop = tqdm(loader, leave=True)
    for batch in loop:
        # initialize calculated gradients (from prev step)
        optim.zero_grad()
        # pull all tensor batches required for training
        input_ids = batch['input_ids'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        next_sentence_label = batch['next_sentence_label'].to(device)
        labels = batch['labels'].to(device)
        # process
        outputs = model(input_ids, attention_mask=attention_mask,
                        token_type_ids=token_type_ids,
                        next_sentence_label=next_sentence_label,
                        labels=labels)
        # extract loss
        loss = outputs.loss
        # calculate loss for every parameter that needs grad update
        loss.backward()
        # update parameters
        optim.step()
        # print relevant info to progress bar
        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())

model.save_pretrained('/C/Users/Lenovo 2022/.spyder-py3/')
tokenizer.save_pretrained('/C/Users/Lenovo 2022/.spyder-py3/')


tokenizer = BertTokenizer.from_pretrained('/C/Users/Lenovo 2022/.spyder-py3/')
model = BertForPreTraining.from_pretrained('/C/Users/Lenovo 2022/.spyder-py3/')


from transformers import pipeline

unmasker = pipeline('fill-mask', model='/C/Users/Lenovo 2022/.spyder-py3/')
unmasker("Joists are connected [MASK] to beams as wall is a vertical surface.")


#SENTENCE-BERT Model
#Repeat proprocessing from line 35 to 48 just to have only sentences as corpus
import scipy

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('bert-base-nli-mean-tokens')

sentences = corpus



print(sentences[:5])     #only for visualization

#embedding the sentences through the model
sentence_embeddings_base = model.encode(sentences)

#visualizing the embeddings for the first five sentences in corpus
for sentence, embedding in zip(sentences[:5], sentence_embeddings_base[:5]):
    print("sentence:", sentence)
    print("Embedding:", embedding)
    print("")
    
    
#querying the model to search out related items

query = ('workmanship')

queries=[query]

query_embeddings = model.encode(queries)

print("Semantic search result")

for query, query_embedding in zip(queries, query_embeddings):
    distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings_base, "cosine")[0]
    
    results = zip(range(len(distances)), distances)
    results = sorted(results,key=lambda x: x[1])
    print(" ")
    print("Query:", query)
    print("\n Top 5 most similar sentences in corpus")
    print(" ")
    for idx, distance in results[:5]:
        print(sentences[idx].strip(), "(Cosine Scorce: %.4f)"%(1-distance))
