Created on Sun Nov 20 23:46:08 2022

@author: Odinakachukwu
"""
#Converted the PDF document to Text File

import PyPDF2
import pdfplumber
file = "BuildingDoc.pdf"
pdfobject= open(file,"rb")
pdfreader = PyPDF2.PdfFileReader(pdfobject)
pages=pdfreader.numPages
print(pages)

with pdfplumber.open(file) as pdf:
    for i in range(0,pages):
        page = pdf.pages[i]
        print(f"extract page number:{page}")
        text = f"{page.extract_text()}\n"
        book = open("HMBulding.text", "a",encoding='utf-8')
        book.writelines(text)
        print("saved to text")
book.close()


import nltk
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd 
import re
from collections import Counter


#read the converted text into python
with open(r'C:\Users\Lenovo 2022\HMBulding.text', "r", encoding='utf-8', errors='ignore') as f:
    contents = f.read()
#The corpus was broken down into sentences
corpus = nltk.sent_tokenize(contents)
#print(corpus)

#Preprocessing Commenced using regular expression
for i in range(len(corpus)):
    corpus [i] = corpus [i].lower()    #converts all Uppercase letters to small letters
    corpus [i] = re.sub(r'\W',' ',corpus [i])  #removed empty spaces
    corpus [i] = re.sub(r'\s+',' ',corpus [i]) #removed punctuation marks
    corpus [i] = re.sub(r'[^A-Za-z]',' ',corpus [i]) #removed special characters like @
    shortword = re.compile(r'\W*\b\w{1}\b') # remove short words
    corpus [i] = re.sub(shortword,' ',corpus [i])
#print(corpus)


#removing stop words from corpus and saving output text to file
from nltk.corpus import stopwords
stopwords.words('english')
stop_words = set(stopwords.words('english'))
file1=open("HMbuildingclean.text","a",encoding='utf-8')
for sentence in corpus:
    tokens = nltk.word_tokenize(sentence)  #Tokenization
    for token in tokens:
        token=''.join([i for i in token if not i.isdigit()])
        if not token in stop_words:
            file1.write(" "+token)
file1.close()

#Applying our word2vec model
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
from gensim import models, matutils
from gensim.models import Word2Vec


input_filename= r'C:\Users\Lenovo 2022\HMbuildingclean.text' #text file to train on
model_filename= r'C:\Users\Lenovo 2022\HMbuildingclean.model'# name for saving trained model

#train using skip gram
skip_gram=True

#create vocabulary
print('building vocabulary...')
model = models.Word2Vec()
sentences = models.word2vec.LineSentence(input_filename)
model.build_vocab(sentences)
bigram_transformer = models.Phrases(sentences)
model = models.Word2Vec(bigram_transformer[sentences], vector_size=200, min_count=5, window=6)



#train word2vec model
print('training model....')
if skip_gram:
    model.train(bigram_transformer[sentences], total_examples = model.corpus_count, epochs = 5)
else:
    model.train(bigram_transformer[sentences], total_examples =model.corpus_count, epochs =model.iter,vector_size=200, min_count=5, window=6)

#and save the trained model
print('_saving model...')
model.save(model_filename)

#Exiting
print('Done!')

#testing word2vec word relationships
print(model.wv.most_similar(positive=['foundation','excavated'],negative=['ceiling']))

#Printing the total words in the model
print(model.corpus_total_words)

#visualizing the vector representation around the word 'heating'
print(model.wv.similar_by_vector('heating'))

#visualizing the words around heating
print(model.wv.similar_by_word('heating'))

#end for word2vec

#Interactive tools for Matplotlib to visualize word vectors in a plot
import numpy as np
import matplotlib.pyplot as plt

from sklearn.manifold import TSNE
from gensim.models import Word2Vec

print('loading the model.....')

model_filename= r'C:\Users\Lenovo 2022\HMbuildingclean.model'

model= Word2Vec.load(model_filename)

def display_closestwords_tsnescatterplot(model, word):
    
    arr = np.empty((0,200), dtype='f')
    word_labels = [word]
    
    #get close words
    close_words = model.wv.most_similar(word)
    
    #add the vector for each of the closest words in the array
    arr = np.append(arr, np.array(model.wv.__getitem__([word])), axis=0)
    for wrd_score in close_words:
        wrd_vector = model.wv.__getitem__([wrd_score[0]])
        word_labels.append(wrd_score[0])
        arr=np.append(arr, np.array(model.wv.__getitem__([word])), axis=0)
        
        # find tsne coords for 2 dimensions
    
    tsne = TSNE(n_components=2, random_state=0)
    np.set_printoptions(suppress=True)
    Y = tsne.fit_transform(arr)
    
    x_coords = Y[:, 0]
    y_coords = Y[:, 1]
    
    # Display scatter plot
    plt.scatter(x_coords, y_coords)
    
    for label, x, y, in zip(word_labels, x_coords, y_coords):
        plt.annotate(label, xy=(x,y), xytext=(0, 0), textcoords='offset points')
    plt.xlim(x_coords.min()+0.00009, x_coords.max()+0.00009)
    plt.ylim(y_coords.min()+0.00009, y_coords.max()+0.00009)
    plt.show()

display_closestwords_tsnescatterplot(model, 'roof')  #plots the words around roof

#BERT

from transformers import pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased')
unmasker("code of practice workmanship on [MASK] sites")


#SENTENCE-BERT Model
#Repeat proprocessing from line 35 to 48 just to have only sentences as corpus
import scipy

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('bert-base-nli-mean-tokens')

sentences = corpus



print(sentences[:5])     #only for visualization

#embedding the sentences through the model
sentence_embeddings_base = model.encode(sentences)

#visualizing the embeddings for the first five sentences in corpus
for sentence, embedding in zip(sentences[:5], sentence_embeddings_base[:5]):
    print("sentence:", sentence)
    print("Embedding:", embedding)
    print("")
    
    
#querying the model to search out related items

query = ('workmanship')

queries=[query]

query_embeddings = model.encode(queries)

print("Semantic search result")

for query, query_embedding in zip(queries, query_embeddings):
    distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings_base, "cosine")[0]
    
    results = zip(range(len(distances)), distances)
    results = sorted(results,key=lambda x: x[1])
    print(" ")
    print("Query:", query)
    print("\n Top 5 most similar sentences in corpus")
    print(" ")
    for idx, distance in results[:5]:
        print(sentences[idx].strip(), "(Cosine Scorce: %.4f)"%(1-distance))
